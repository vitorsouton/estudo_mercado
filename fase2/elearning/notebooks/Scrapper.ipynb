{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e63d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import requests\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import warnings; warnings.filterwarnings(action='ignore')\n",
    "from urllib.parse import urlparse\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf83e6",
   "metadata": {},
   "source": [
    "# RECLAME AQUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fcfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'}\n",
    "\n",
    "def get_complains(company_id, company_name, n_complains):\n",
    "\n",
    "    complains = {'company_name': [], 'description': [], 'href': [], 'full_complain': []}\n",
    "    \n",
    "    for n in range(0, n_complains, 10):\n",
    "        url = f'https://iosearch.reclameaqui.com.br/raichu-io-site-search-v1/query/companyComplains/10/{n}'\n",
    "        res = requests.get(url, params={'company': company_id}, headers=HEADERS).json()\n",
    "\n",
    "        for x in range(10):\n",
    "            data = res['complainResult']['complains']['data']\n",
    "            \n",
    "            description = data[x]['description']\n",
    "            soup = BeautifulSoup(description, 'lxml').text\n",
    "            complains['description'].append(soup)\n",
    "            \n",
    "            id_ = data[x]['id']\n",
    "            title = data[x]['title']\n",
    "            clean_title = title.translate(str.maketrans('', '', string.punctuation))\n",
    "            href = clean_title.lower().strip() + '_' + id_\n",
    "            href = unidecode(href.replace(' ', '-'))\n",
    "            complains['href'].append(href)\n",
    "            \n",
    "            complains['company_name'].append(company_name)\n",
    "            \n",
    "            complains['full_complain'].append('')\n",
    "            \n",
    "    df = pd.DataFrame(complains)\n",
    "    \n",
    "    for i, company_name, href in zip(df.index, df.company_name, df.href):\n",
    "        try:\n",
    "            url = f\"https://www.reclameaqui.com.br/{company_name}/{href}/\"\n",
    "            res = requests.get(url, headers=HEADERS)\n",
    "\n",
    "            soup = BeautifulSoup(res.text, 'lxml')\n",
    "            complain = soup.find('p', {'class': 'lzlu7c-17 fXwQIB'}).text\n",
    "            df.iloc[i]['full_complain'] = complain\n",
    "        except:\n",
    "            df.drop(index=i, inplace=True)\n",
    "\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652e367",
   "metadata": {},
   "source": [
    "# INFOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f746f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac73b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    \n",
    "    res = requests.get(url).content\n",
    "    soup = BeautifulSoup(res, 'html.parser')\n",
    "    proxies = []\n",
    "    \n",
    "    for r in soup.find('table', attrs={'class': 'table table-striped table-bordered'}).find_all('tr')[1:]:\n",
    "        tds = r.find_all('td')\n",
    "        try:\n",
    "            ip = tds[0].text.strip()\n",
    "            port = tds[1].text.strip()\n",
    "            host = f'{ip}:{port}'\n",
    "            proxies.append(host)\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "601ca962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFORMACOES DO TRAFEGO\n",
    "def get_info(company, proxies_list):\n",
    "    p = '/home/vitor/drivers/'\n",
    "    \n",
    "    firefox_options = Options()\n",
    "    firefox_options.add_argument('--headless')\n",
    "    \n",
    "    if proxies_list:\n",
    "        proxies = proxies_list.copy()\n",
    "        proxy = choice(proxies)\n",
    "\n",
    "        capabilities = dict(DesiredCapabilities.FIREFOX)\n",
    "        capabilities['proxy'] = {\n",
    "            'http': proxy,\n",
    "            'https': proxy,\n",
    "            'proxyType': 'MANUAL',\n",
    "            'socksProxy': proxy,\n",
    "            'socksVersion': 5,\n",
    "            'ftpProxy': proxy,\n",
    "            'noProxy': 'localhost,127.0.0.1',\n",
    "            'class': 'org.openqa.selenium.Proxy',\n",
    "            'autodetect': False\n",
    "        }\n",
    "\n",
    "\n",
    "        driver = webdriver.Firefox(p, options=firefox_options, desired_capabilities=capabilities)\n",
    "        \n",
    "    else:\n",
    "        driver = webdriver.Firefox(p, options=firefox_options)\n",
    "    \n",
    "    sleep(2)\n",
    "    \n",
    "    driver.get(f'https://www.similarweb.com/website/{company}/')\n",
    "    \n",
    "    result = {\n",
    "        'company_name': [company],\n",
    "        'brazilian_visitors': [-1]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        comp_info = driver.find_elements(By.CSS_SELECTOR, 'div.wa-overview__column:nth-child(6)')\n",
    "        \n",
    "        country = comp_info[0].text.split('\\n')[7].split(',')[0]\n",
    "        result['hq_country'] = country\n",
    "        \n",
    "        info = driver.find_elements(By.CLASS_NAME, 'engagement-list__item')\n",
    "\n",
    "        bounce_rate = float(info[1].text.split('\\n')[1][:-1]) / 100\n",
    "        result['bounce_rate'] = [round(bounce_rate, 3)]\n",
    "\n",
    "        visit_dur = int(info[3].text.split('\\n')[1][3:5])\n",
    "        result['visit_duration'] = [visit_dur]\n",
    "\n",
    "        if info[0].text.split('\\n')[1][-1] == 'M':\n",
    "            total_visits = float(info[0].text.split('\\n')[1][:-1]) * 1_000_000\n",
    "        else:\n",
    "            total_visits = float(info[0].text.split('\\n')[1][:-1]) * 1_000\n",
    "\n",
    "        country = driver.find_elements(By.CLASS_NAME, 'wa-geography__country-name')\n",
    "        pct = driver.find_elements(By.CLASS_NAME, 'wa-geography__country-traffic-value')\n",
    "        for c, p in zip(country, pct):\n",
    "            c_text = c.text\n",
    "            p_text = p.text\n",
    "\n",
    "            if c_text == 'Brazil':\n",
    "                percentage = float(p_text[:-1]) / 100\n",
    "                br_visits = round(total_visits * percentage)\n",
    "                result['brazilian_visitors'] = [br_visits]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        sex = driver.find_elements(By.CLASS_NAME, 'wa-demographics__gender-legend')[0].text\n",
    "        sex_text = sex.split('\\n')\n",
    "        male = float(sex_text[3][:-1]) / 100\n",
    "        female = float(sex_text[1][:-1]) / 100\n",
    "        result['male'] = [round(male, 3)]\n",
    "        result['female'] = [round(female, 3)]\n",
    "\n",
    "        svg_eles = driver.find_elements(By.CLASS_NAME, 'highcharts-root')\n",
    "        svg = svg_eles[3]\n",
    "        svg_text = svg.text\n",
    "\n",
    "        idades = svg_text.split('\\n')[:6]\n",
    "        labels = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "        for i, l in zip(idades, labels):\n",
    "            i_pct = round((float(i[:-1]) / 100), 3)\n",
    "            result[l] = [i_pct]\n",
    "    except IndexError:\n",
    "        print('In the loop')\n",
    "        os.system('nordvpn c > /dev/null 2>&1')\n",
    "        df = get_info(company, proxies_list=proxies)\n",
    "        print('Out of loop')\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        df = pd.DataFrame(result)\n",
    "        sleep(0.5)\n",
    "\n",
    "        return df\n",
    "        \n",
    "    driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "    \n",
    "    sleep(0.5)\n",
    "    print(f'Done {company}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf7576f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empresas = pd.read_csv('../data/elearning_companies.csv')\n",
    "empresas = empresas[['Website']]\n",
    "empresas.dropna(inplace=True)\n",
    "empresas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c23dbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.icould.com</td>\n",
       "      <td>icould.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.engineeringmanagement.info</td>\n",
       "      <td>engineeringmanagement.info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.babbel.com</td>\n",
       "      <td>babbel.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.management-academy.us</td>\n",
       "      <td>management-academy.us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.w3schools.com</td>\n",
       "      <td>w3schools.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Website                      domain\n",
       "0                  http://www.icould.com                  icould.com\n",
       "1  http://www.engineeringmanagement.info  engineeringmanagement.info\n",
       "2                  http://www.babbel.com                  babbel.com\n",
       "3       http://www.management-academy.us       management-academy.us\n",
       "4               http://www.w3schools.com               w3schools.com"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empresas['domain'] = [0] * empresas.shape[0]\n",
    "empresas['domain'] = [urlparse(site).netloc[4:] for site in empresas.Website]\n",
    "empresas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c47f6e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done icould.com\n",
      "Done engineeringmanagement.info\n",
      "Done babbel.com\n",
      "Done w3schools.com\n",
      "Done jobzeg.com\n",
      "Done datacamp.com\n",
      "Done codecademy.com\n",
      "Done edmentum.com\n",
      "Done onlinelearningconsortium.org\n",
      "Done academicpartnerships.com\n",
      "Done lynda.com\n",
      "Done onlinetutorials.org\n",
      "Done interaction-design.org\n",
      "In the loop\n",
      "Out of loop\n",
      "Done theamericancollege.edu\n",
      "Done tutorme.com\n",
      "Done labster.com\n",
      "Done toppr.com\n",
      "Done codingninjas.com\n",
      "Done skillshare.com\n",
      "Done ageoflearning.com\n",
      "Done beginlearning.co\n",
      "Done explorelearning.com\n",
      "Done nearpod.com\n",
      "Done lynda.com\n",
      "Done busuu.com\n",
      "Done vitalsource.com\n",
      "In the loop\n",
      "Done masterclass.com\n",
      "Out of loop\n",
      "Done cengagegroup.com\n",
      "Done oreilly.com\n",
      "Done learninga-z.com\n",
      "Done rosalind.info\n",
      "Done byjus.com\n",
      "Done onlinecourses.ooo\n",
      "Done freecoursesandbooks.net\n",
      "Done extramarks.com\n",
      "Done corporatefinanceinstitute.com\n",
      "Done vectorsolutions.com\n",
      "Done opensesame.com\n",
      "Done hsi.com\n",
      "Done unacademy.com\n",
      "Done bizmates.jp\n",
      "Done freecodecamp.org\n",
      "Done byu.edu\n",
      "Done uninove.br\n",
      "Done udacity.com\n",
      "Done gofluent.com\n",
      "Done thoughtindustries.com\n",
      "Done planetspark.in\n",
      "Done skill-lync.com\n",
      "Done lynda.com\n",
      "Done datascienceacademy.com.br\n",
      "Done codingclub.tech\n",
      "Done vedantu.com\n",
      "Done unipar.br\n",
      "Done coursera.org\n",
      "Done skillsoft.com\n",
      "Done hmhco.com\n",
      "Done openenglish.com\n",
      "Done geeksgod.com\n",
      "Done udemy.com\n",
      "In the loop\n",
      "Done beacademy.com.br\n",
      "Out of loop\n",
      "Done ascendlearning.com\n",
      "Done relias.com\n",
      "Done lidolearning.com\n",
      "Done cambiumlearning.com\n",
      "Done xxx.com\n",
      "Done whitehatjr.com\n",
      "Done e3melbusiness.com\n",
      "Done artofproblemsolving.com\n",
      "Done waterford.org\n",
      "Done upgrad.com\n",
      "Done lynda.com\n",
      "In the loop\n",
      "Done 10minuteschool.com\n",
      "Out of loop\n",
      "Done newsela.com\n",
      "Done elevatek12.com\n",
      "Done myon.com\n",
      "Done institutosc.com.br\n",
      "Done k12.com\n",
      "Done imaginelearning.com\n",
      "Done ethicalhackersacademy.com\n",
      "Done ixl.com\n",
      "Done dreambox.com\n",
      "Done scaler.com\n",
      "Done marieforleo.com\n",
      "In the loop\n",
      "Done rosalind.info\n",
      "Out of loop\n",
      "Done marieforleo.com\n",
      "CPU times: user 40.1 s, sys: 2.15 s, total: 42.2 s\n",
      "Wall time: 21min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "proxies_list = get_proxies()\n",
    "\n",
    "data = pd.concat((get_info(comp, proxies_list=proxies_list) for comp in empresas.domain), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c6399a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/elearning_info.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
